{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnEI311UTlP6"
      },
      "source": [
        "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
        "\n",
        "<i>Licensed under the MIT License.</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uQhgoOiHTovR",
        "outputId": "540bb9e6-14cb-458e-83d5-fdffd6eed223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\numpy\\\\compat\\\\py3k.py'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Using cached https://files.pythonhosted.org/packages/af/f2/b9ebc1355c2d48c5823f9a33bfd91d885c1499846080eebc4ea21f5e509d/tensorflow-2.9.1-cp37-cp37m-win_amd64.whl\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/ff/ff/f25909606aed26981a8bd6d263f89d64a20ca5e5316e6aafb4c75d9ec8ae/keras-2.9.0-py2.py3-none-any.whl\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (19.2)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/ee/0d/23812e6ce63b3d87c39bc9fee83e28c499052fa83fddddd7daea21a6d620/tensorboard-2.9.1-py3-none-any.whl\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/c3/a4/ca66ec75620d7e0dc488a22eced633fe5233e82df0c552be3dbf04bfe2d4/tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-win_amd64.whl\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.1.0)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (41.4.0)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/b7/70/e4b10c1b76adaf734317c82ab7f983e8f614db73d999c13c1329460a503c/grpcio-1.47.0-cp37-cp37m-win_amd64.whl\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/a5/b8/fc74a554a6fc7f26744c883ebfe405cf49c1f1320f13ee874aee47c70e1d/absl_py-1.2.0-py3-none-any.whl\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/b3/41/5e2dfb15fdae38be5dd62be30f9fd40c57e50bd56ee2c276f6230c57a3ad/protobuf-3.19.4-cp37-cp37m-win_amd64.whl\n",
            "Collecting numpy>=1.20 (from tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/97/9f/da37cc4a188a1d5d203d65ab28d6504e17594b5342e0c1dc5610ee6f4535/numpy-1.21.6-cp37-cp37m-win_amd64.whl\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.33.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (2.4.2)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/b1/0e/0636cc1448a7abc444fb1b3a63655e294e0d2d49092dc3de05241be6d43c/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/63/0e4cb01bc4fe5c62ce63587c9471645afdb8a1a3dc2fa1fe2b6a3fbb78f5/Werkzeug-2.2.0-py3-none-any.whl (232kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/7b/17/0b14f55fc8ff002b92e2deb796dd9e28a65ca1a6272d9d844e99051afb67/google_auth-2.9.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.22.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/74/69/5747a957f95e2e1d252ca41476ae40ce79d70d38151d2e494feb7722860c/tensorboard_data_server-0.6.1-py3-none-any.whl\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/86/be/ad281f7a3686b38dd8a307fa33210cdf2130404dfef668a37a4166d737ca/Markdown-3.4.1-py3-none-any.whl\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/e0/68/e8ecfac5dd594b676c23a7f07ea34c197d7d69b3313afdf8ac1b0a9905a2/tensorboard_plugin_wit-1.8.1-py3-none-any.whl\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/6f/bb/5deac77a9af870143c684ab46a7934038a53eb4aa975bc0687ed6ca2c610/requests_oauthlib-1.3.1-py2.py3-none-any.whl\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/38/f422a81b41bdac48f1d19c45f6e203c04bc45d2c35505873fdecdddee1ec/MarkupSafe-2.1.1-cp37-cp37m-win_amd64.whl\n",
            "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\" (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/68/aa/5fc646cae6e997c3adf3b0a7e257cda75cff21fcba15354dffd67789b7bb/cachetools-5.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2019.9.11)\n",
            "Collecting importlib-metadata>=4.4; python_version < \"3.10\" (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/d2/a2/8c239dc898138f208dd14b441b196e7b3032b94d3137d9d8453e186967fc/importlib_metadata-4.12.0-py3-none-any.whl\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/1d/46/5ee2475e1b46a26ca0fa10d3c1d479577fde6ee289f8c6aa6d7ec33e31fd/oauthlib-3.2.0-py3-none-any.whl\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow)\n",
            "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (7.2.0)\n",
            "Installing collected packages: gast, numpy, keras-preprocessing, keras, astunparse, google-pasta, oauthlib, requests-oauthlib, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, protobuf, MarkupSafe, werkzeug, absl-py, tensorboard-data-server, importlib-metadata, markdown, tensorboard-plugin-wit, grpcio, tensorboard, tensorflow-io-gcs-filesystem, flatbuffers, opt-einsum, termcolor, tensorflow\n",
            "  Found existing installation: numpy 1.16.5\n",
            "    Uninstalling numpy-1.16.5:\n",
            "Collecting papermill\n",
            "  Using cached https://files.pythonhosted.org/packages/93/74/15fb040b880a07d7caf32af20a367b4747e2b6f0802e015891bca876e88c/papermill-2.3.4-py3-none-any.whl\n",
            "Collecting nbclient>=0.2.0 (from papermill)\n",
            "  Using cached https://files.pythonhosted.org/packages/68/88/a3f13adcf5708cf0d5f9c4c95e12d1527f6498d87b30d463b588bb466c15/nbclient-0.6.6-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.32.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from papermill) (4.36.1)\n",
            "Collecting nbformat>=5.1.2 (from papermill)\n",
            "  Using cached https://files.pythonhosted.org/packages/2f/9a/97151abb954af0cc5d0e3ff2eb7b6d96704a317ac2c0ce0cc76cef003991/nbformat-5.4.0-py3-none-any.whl\n",
            "Collecting ansiwrap (from papermill)\n",
            "  Using cached https://files.pythonhosted.org/packages/03/50/43e775a63e0d632d9be3b3fa1c9b2cbaf3b7870d203655710a3426f47c26/ansiwrap-0.8.4-py2.py3-none-any.whl\n",
            "Collecting tenacity (from papermill)\n",
            "  Using cached https://files.pythonhosted.org/packages/f2/a5/f86bc8d67c979020438c8559cc70cfe3a1643fd160d35e09c9cca6a09189/tenacity-8.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: entrypoints in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from papermill) (0.4)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from papermill) (7.0)\n",
            "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from papermill) (5.1.2)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from papermill) (2.22.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.5 in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from nbclient>=0.2.0->papermill) (7.1.2)\n",
            "Collecting traitlets>=5.2.2 (from nbclient>=0.2.0->papermill)\n",
            "  Using cached https://files.pythonhosted.org/packages/83/a9/1059771062cb80901c34a4dea020e76269412e69300b4ba12e3356865ad8/traitlets-5.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from nbclient>=0.2.0->papermill) (1.5.4)\n",
            "Requirement already satisfied: jupyter-core in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from nbformat>=5.1.2->papermill) (4.9.1)\n",
            "Requirement already satisfied: fastjsonschema in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=5.1.2->papermill) (2.16.1)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: jupyter-console 6.0.0 has requirement prompt_toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 3.0.26 which is incompatible.\n",
            "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\traitlets-4.3.3.dist-info\\\\COPYING.md'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: jsonschema>=2.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=5.1.2->papermill) (3.0.2)\n",
            "Collecting textwrap3>=0.9.2 (from ansiwrap->papermill)\n",
            "  Using cached https://files.pythonhosted.org/packages/77/9c/a53e561d496ee5866bbeea4d3a850b3b545ed854f8a21007c1e0d872e94d/textwrap3-0.9.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->papermill) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->papermill) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->papermill) (1.24.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->papermill) (2019.9.11)\n",
            "Requirement already satisfied: tornado>=4.1 in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill) (6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill) (2.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill) (22.3.0)\n",
            "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-core->nbformat>=5.1.2->papermill) (303)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.15.4)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (41.4.0)\n",
            "Requirement already satisfied: six>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (1.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (19.2.0)\n",
            "Installing collected packages: traitlets, nbformat, nbclient, textwrap3, ansiwrap, tenacity, papermill\n",
            "  Found existing installation: traitlets 4.3.3\n",
            "    Uninstalling traitlets-4.3.3:\n",
            "Collecting scrapbook\n",
            "  Using cached https://files.pythonhosted.org/packages/27/dc/68f9c96997dffbf3632bebe0d88077a519aa2a74585834e84d6690243825/scrapbook-0.5.0-py3-none-any.whl\n",
            "Collecting papermill (from scrapbook)\n",
            "  Using cached https://files.pythonhosted.org/packages/93/74/15fb040b880a07d7caf32af20a367b4747e2b6f0802e015891bca876e88c/papermill-2.3.4-py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema in c:\\programdata\\anaconda3\\lib\\site-packages (from scrapbook) (3.0.2)\n",
            "Collecting pyarrow (from scrapbook)\n",
            "  Using cached https://files.pythonhosted.org/packages/d7/a3/46b059a7d8e382df1c704b04cab5c9f1b8c5dfeda76eb36d9a64b38d8097/pyarrow-8.0.0-cp37-cp37m-win_amd64.whl\n",
            "Requirement already satisfied: ipython in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from scrapbook) (7.31.1)\n",
            "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from scrapbook) (0.25.1)\n",
            "Collecting nbclient>=0.2.0 (from papermill->scrapbook)\n",
            "  Using cached https://files.pythonhosted.org/packages/68/88/a3f13adcf5708cf0d5f9c4c95e12d1527f6498d87b30d463b588bb466c15/nbclient-0.6.6-py3-none-any.whl\n",
            "Collecting tenacity (from papermill->scrapbook)\n",
            "  Using cached https://files.pythonhosted.org/packages/f2/a5/f86bc8d67c979020438c8559cc70cfe3a1643fd160d35e09c9cca6a09189/tenacity-8.0.1-py3-none-any.whl\n",
            "Collecting nbformat>=5.1.2 (from papermill->scrapbook)\n",
            "  Using cached https://files.pythonhosted.org/packages/2f/9a/97151abb954af0cc5d0e3ff2eb7b6d96704a317ac2c0ce0cc76cef003991/nbformat-5.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from papermill->scrapbook) (5.1.2)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from papermill->scrapbook) (7.0)\n",
            "Collecting ansiwrap (from papermill->scrapbook)\n",
            "  Using cached https://files.pythonhosted.org/packages/03/50/43e775a63e0d632d9be3b3fa1c9b2cbaf3b7870d203655710a3426f47c26/ansiwrap-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: entrypoints in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from papermill->scrapbook) (0.4)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from papermill->scrapbook) (2.22.0)\n",
            "Requirement already satisfied: tqdm>=4.32.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from papermill->scrapbook) (4.36.1)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema->scrapbook) (41.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema->scrapbook) (19.2.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema->scrapbook) (0.15.4)\n",
            "Requirement already satisfied: six>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema->scrapbook) (1.12.0)\n",
            "Collecting numpy>=1.16.6 (from pyarrow->scrapbook)\n",
            "  Using cached https://files.pythonhosted.org/packages/97/9f/da37cc4a188a1d5d203d65ab28d6504e17594b5342e0c1dc5610ee6f4535/numpy-1.21.6-cp37-cp37m-win_amd64.whl\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from ipython->scrapbook) (0.18.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from ipython->scrapbook) (3.0.26)\n",
            "Requirement already satisfied: decorator in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from ipython->scrapbook) (5.1.1)\n",
            "Requirement already satisfied: backcall in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from ipython->scrapbook) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from ipython->scrapbook) (0.1.3)\n",
            "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->scrapbook) (0.4.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->scrapbook) (4.3.3)\n",
            "Requirement already satisfied: pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->scrapbook) (2.4.2)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from ipython->scrapbook) (0.7.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->scrapbook) (2019.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->scrapbook) (2.8.0)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from nbclient>=0.2.0->papermill->scrapbook) (1.5.4)\n",
            "Requirement already satisfied: jupyter-client>=6.1.5 in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from nbclient>=0.2.0->papermill->scrapbook) (7.1.2)\n",
            "Requirement already satisfied: fastjsonschema in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=5.1.2->papermill->scrapbook) (2.16.1)\n",
            "Requirement already satisfied: jupyter-core in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from nbformat>=5.1.2->papermill->scrapbook) (4.9.1)\n",
            "Collecting textwrap3>=0.9.2 (from ansiwrap->papermill->scrapbook)\n",
            "  Using cached https://files.pythonhosted.org/packages/77/9c/a53e561d496ee5866bbeea4d3a850b3b545ed854f8a21007c1e0d872e94d/textwrap3-0.9.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->papermill->scrapbook) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->papermill->scrapbook) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->papermill->scrapbook) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->papermill->scrapbook) (1.24.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from jedi>=0.16->ipython->scrapbook) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->scrapbook) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in c:\\programdata\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->scrapbook) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4.1 in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (22.3.0)\n",
            "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" in c:\\users\\morja\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-core->nbformat>=5.1.2->papermill->scrapbook) (303)\n",
            "Installing collected packages: nbformat, nbclient, tenacity, textwrap3, ansiwrap, papermill, numpy, pyarrow, scrapbook\n",
            "  Found existing installation: nbformat 4.4.0\n",
            "    Uninstalling nbformat-4.4.0:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: nbformat 5.4.0 has requirement traitlets>=5.1, but you'll have traitlets 4.3.3 which is incompatible.\n",
            "ERROR: nbclient 0.6.6 has requirement traitlets>=5.2.2, but you'll have traitlets 4.3.3 which is incompatible.\n",
            "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\nbformat-4.4.0.dist-info\\\\entry_points.txt'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tempfile\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  ERROR: Could not find a version that satisfies the requirement tempfile (from versions: none)\n",
            "ERROR: No matching distribution found for tempfile\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install papermill\n",
        "!pip install scrapbook\n",
        "!pip install tempfile\n",
        "!pip install recommenders\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QukXUaNHTlQD"
      },
      "source": [
        "# Sequential Recommender Quick Start\n",
        "\n",
        "### Example: SLi_Rec : Adaptive User Modeling with Long and Short-Term Preferences for Personailzed Recommendation\n",
        "Unlike a general recommender such as Matrix Factorization or xDeepFM (in the repo) which doesn't consider the order of the user's activities, sequential recommender systems take the sequence of the user behaviors as context and the goal is to predict the items that the user will interact in a short time (in an extreme case, the item that the user will interact next).\n",
        "\n",
        "This notebook aims to give you a quick example of how to train a sequential model based on a public Amazon dataset. Currently, we can support NextItNet \\[4\\], GRU4Rec \\[2\\], Caser \\[3\\], A2SVD \\[1\\], SLi_Rec \\[1\\], and SUM \\[5\\]. Without loss of generality, this notebook takes [SLi_Rec model](https://www.microsoft.com/en-us/research/uploads/prod/2019/07/IJCAI19-ready_v1.pdf) for example.\n",
        "SLi_Rec \\[1\\] is a deep learning-based model aims at capturing both long and short-term user preferences for precise recommender systems. To summarize, SLi_Rec has the following key properties:\n",
        "\n",
        "* It adopts the attentive \"Asymmetric-SVD\" paradigm for long-term modeling;\n",
        "* It takes both time irregularity and semantic irregularity into consideration by modifying the gating logic in LSTM.\n",
        "* It uses an attention mechanism to dynamic fuse the long-term component and short-term component.\n",
        "\n",
        "In this notebook, we test SLi_Rec on a subset of the public dataset: [Amazon_reviews](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz) and [Amazon_metadata](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Movies_and_TV.json.gz)\n",
        "\n",
        "This notebook is tested under TF 2.6. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui9UxGjkTlQF"
      },
      "source": [
        "## 0. Global Settings and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "tshs7Nd_TlQG",
        "outputId": "2d4edaba-61aa-4f74-d7bb-b13c321d335e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-083306dbfc39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrecommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeprec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential_iterator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequentialIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msli_rec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSLI_RECModel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSeqModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'main.sli_rec'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import papermill as pm\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.utils.constants import SEED\n",
        "from recommenders.models.deeprec.deeprec_utils import (\n",
        "    prepare_hparams\n",
        ")\n",
        "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
        "from recommenders.datasets.download_utils import maybe_download\n",
        "\n",
        "\n",
        "# from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
        "####  to use the other model, use one of the following lines:\n",
        "# from recommenders.models.deeprec.models.sequential.asvd import A2SVDModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.caser import CaserModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.gru4rec import GRU4RecModel as SeqModel\n",
        "# from recommenders.models.deeprec.models.sequential.sum import SUMModel as SeqModel\n",
        "\n",
        "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
        "\n",
        "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
        "from sli_rec import SLI_RECModel as SeqModel\n",
        "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS4vxQsUTlQM"
      },
      "outputs": [],
      "source": [
        "##  ATTENTION: change to the corresponding config file, e.g., caser.yaml for CaserModel, sum.yaml for SUMModel\n",
        "# yaml_file = '../../recommenders/models/deeprec/config/sli_rec.yaml'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "yaml_file = '/content/drive/My Drive/Colab Notebooks/sli_rec.yaml'  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeWYzdMtTlQO"
      },
      "source": [
        "#### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFOrM7R3TlQP",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 400\n",
        "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
        "\n",
        "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6KLJCo9TlQR"
      },
      "source": [
        "##  1. Input data format\n",
        "The input data contains 8 columns, i.e.,   `<label> <user_id> <item_id> <category_id> <timestamp> <history_item_ids> <history_cateory_ids> <hitory_timestamp>`  columns are seperated by `\"\\t\"`.  item_id and category_id denote the target item and category, which means that for this instance, we want to guess whether user user_id will interact with item_id at timestamp. `<history_*>` columns record the user behavior list up to `<timestamp>`, elements are separated by commas.  `<label>` is a binary value with 1 for positive instances and 0 for negative instances.  One example for an instance is: \n",
        "\n",
        "`1       A1QQ86H5M2LVW2  B0059XTU1S      Movies  1377561600      B002ZG97WE,B004IK30PA,B000BNX3AU,B0017ANB08,B005LAIHW2  Movies,Movies,Movies,Movies,Movies   1304294400,1304812800,1315785600,1316304000,1356998400` \n",
        "\n",
        "In data preprocessing stage, we have a script to generate some ID mapping dictionaries, so user_id, item_id and category_id will be mapped into interager index starting from 1. And you need to tell the input iterator where is the ID mapping files are. (For example, in the next section, we have some mapping files like user_vocab, item_vocab, and cate_vocab).  The data preprocessing script is at [recommenders/dataset/amazon_reviews.py](../../recommenders/dataset/amazon_reviews.py), you need to call the `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` function. Note that ID vocabulary only creates from the train_file, so the new IDs in valid_file or test_file will be regarded as unknown IDs and assigned with a defualt 0 index.\n",
        "\n",
        "Only the SLi_Rec model is time-aware. For the other models, you can just pad some meaningless timestamp in the data files to fill up the format, the models will ignore these columns.\n",
        "\n",
        "We use Softmax to the loss function. In training and evalution stage, we group 1 positive instance with `num_ngs` negative instances. Pair-wise ranking can be regarded as a special case of softmax ranking, where `num_ngs` is set to 1. \n",
        "\n",
        "More specifically, for training and evalation, you need to organize the data file such that each one positive instance is followed by `num_ngs` negative instances. Our program will take `1+num_ngs` lines as a unit for Softmax calculation. `num_ngs` is a parameter you need to pass to the `prepare_hparams`, `fit` and `run_eval` function. `train_num_ngs` in `prepare_hparams` denotes the number of negative instances for training, where a recommended number is 4. `valid_num_ngs` and `num_ngs` in `fit` and `run_eval` denote the number in evalution. In evaluation, the model calculates metrics among the `1+num_ngs` instances. For the `predict` function, since we only need to calcuate a score for each individual instance, there is no need for `num_ngs` setting.  More details and examples will be provided in the following sections.\n",
        "\n",
        "For training stage, if you don't want to prepare negative instances, you can just provide positive instances and set the parameter `need_sample=True, train_num_ngs=train_num_ngs` for function `prepare_hparams`, our model will dynamicly sample `train_num_ngs` instances as negative samples in each mini batch.\n",
        "\n",
        "###  Amazon dataset\n",
        "Now let's start with a public dataset containing product reviews and metadata from Amazon, which is widely used as a benchmark dataset in recommemdation systems field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TAFka7ErWSo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from recommenders.datasets.python_splitters import (\n",
        "    python_random_split, \n",
        "    python_chrono_split, \n",
        "    python_stratified_split\n",
        ")\n",
        "from recommenders.datasets.amazon_reviews import _create_vocab\n",
        "# data_file = pd.read_csv('/content/drive/My Drive/Colab Notebooks/mock_sli_rec.csv')\n",
        "# data_train, data_validate, data_test = python_random_split(data_file, ratio=[0.6, 0.2, 0.2])\n",
        "\n",
        "f1 = open(\"/content/drive/My Drive/Colab Notebooks/user_vocab.pkl\",\"r\")\n",
        "f2 = open(\"/content/drive/My Drive/Colab Notebooks/item_vocab.pkl\",\"r\")\n",
        "f3 = open(\"/content/drive/My Drive/Colab Notebooks/cate_vocab.pkl\",\"r\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqRaAMX3dpYA"
      },
      "outputs": [],
      "source": [
        "data_file = pd.read_csv('/content/drive/My Drive/Colab Notebooks/mock_sli_rec.csv',on_bad_lines='skip',sep=\"\\t\")\n",
        "data_train, data_validate, data_test = python_random_split(data_file, ratio=[0.6, 0.2, 0.2])\n",
        "data_train.to_csv(\"/content/drive/My Drive/Colab Notebooks/train_file.csv\",sep=\"\\t\",index = False,header=None)\n",
        "data_validate.to_csv(\"/content/drive/My Drive/Colab Notebooks/valid_file.csv\",sep=\"\\t\",index = False,header = None)\n",
        "data_test.to_csv(\"/content/drive/My Drive/Colab Notebooks/test_file.csv\",sep=\"\\t\",index = False,header = None)\n",
        "_create_vocab(\"/content/drive/My Drive/Colab Notebooks/mock_sli_rec.csv\",\"/content/drive/My Drive/Colab Notebooks/user_vocab.pkl\",\"/content/drive/My Drive/Colab Notebooks/item_vocab.pkl\",\"/content/drive/My Drive/Colab Notebooks/cate_vocab.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59N8TXMTTo9b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u9fh77DTlQT",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "\n",
        "# for test\n",
        "# train_file = os.path.join(data_path, r'train_data')\n",
        "# valid_file = os.path.join(data_path, r'valid_data')\n",
        "# test_file = os.path.join(data_path, r'test_data')\n",
        "train_file = \"/content/drive/My Drive/Colab Notebooks/train_file.csv\"\n",
        "valid_file = \"/content/drive/My Drive/Colab Notebooks/valid_file.csv\"\n",
        "test_file = \"/content/drive/My Drive/Colab Notebooks/test_file.csv\"\n",
        "user_vocab = \"/content/drive/My Drive/Colab Notebooks/user_vocab.pkl\"\n",
        "item_vocab = \"/content/drive/My Drive/Colab Notebooks/item_vocab.pkl\"\n",
        "cate_vocab = \"/content/drive/My Drive/Colab Notebooks/cate_vocab.pkl\"\n",
        "output_file = \"/content/drive/My Drive/Colab Notebooks/output.txt\"\n",
        "# user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
        "# item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
        "# cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
        "# output_file = os.path.join(data_path, r'output.txt')\n",
        "\n",
        "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
        "meta_name = 'meta_Movies_and_TV.json'\n",
        "reviews_file = os.path.join(data_path, reviews_name)\n",
        "meta_file = os.path.join(data_path, meta_name)\n",
        "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
        "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
        "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
        "sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
        "\n",
        "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
        "\n",
        "if not os.path.exists(train_file):\n",
        "    print(\"Here\")\n",
        "    download_and_extract(reviews_name, reviews_file)\n",
        "    download_and_extract(meta_name, meta_file)\n",
        "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
        "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
        "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZyez3b1TlQW"
      },
      "source": [
        "#### 1.1 Prepare hyper-parameters\n",
        "prepare_hparams() will create a full set of hyper-parameters for model training, such as learning rate, feature number, and dropout ratio. We can put those parameters in a yaml file (a complete list of parameters can be found under our config folder) , or pass parameters as the function's parameters (which will overwrite yaml settings).\n",
        "\n",
        "Parameters hints: <br>\n",
        "`need_sample` controls whether to perform dynamic negative sampling in mini-batch. \n",
        "`train_num_ngs` indicates how many negative instances followed by one positive instances.  <br>\n",
        "Examples: <br>\n",
        "(1) `need_sample=True and train_num_ngs=4`:  There are only positive instances in your training file. Our model will dynamically sample 4 negative instances for each positive instances in mini-batch. Note that if need_sample is set to True, train_num_ngs should be greater than zero. <br>\n",
        "(2) `need_sample=False and train_num_ngs=4`: In your training file, each one positive line is followed by 4 negative lines. Note that if need_sample is set to False, you must provide a traiing file with negative instances, and train_num_ngs should match the number of negative number in your training file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX87QarzTlQX",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "### NOTE:  \n",
        "### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n",
        "hparams = prepare_hparams(yaml_file, \n",
        "                          embed_l2=0., \n",
        "                          layer_l2=0., \n",
        "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
        "                          epochs=EPOCHS,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          show_step=20,\n",
        "                          MODEL_DIR=os.path.join(data_path, \"model/\"),\n",
        "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/\"),\n",
        "                          user_vocab=user_vocab,\n",
        "                          item_vocab=item_vocab,\n",
        "                          cate_vocab=cate_vocab,\n",
        "                          need_sample=True,\n",
        "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owfeXt2DTlQY"
      },
      "source": [
        "#### 1.2 Create data loader\n",
        "Designate a data iterator for the model. All our sequential models use SequentialIterator. \n",
        "data format is introduced aboved. \n",
        "\n",
        "<br>Validation and testing data are files after negative sampling offline with the number of `<num_ngs>` and `<test_num_ngs>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO2b2NDeTlQZ"
      },
      "outputs": [],
      "source": [
        "input_creator = SequentialIterator\n",
        "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
        "#input_creator = NextItNetIterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZRiGqbwTlQa"
      },
      "source": [
        "## 2. Create model\n",
        "When both hyper-parameters and data iterator are ready, we can create a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq2LR3vFTlQb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "\n",
        "## sometimes we don't want to train a model from scratch\n",
        "## then we can load a pre-trained model like this: \n",
        "#model.load_model(r'your_model_path')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uu2arqzTlQb"
      },
      "source": [
        "Now let's see what is the model's performance at this point (without starting training):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBK_u5LkdHt_"
      },
      "outputs": [],
      "source": [
        "# with open(\"/content/drive/My Drive/Colab Notebooks/mock_sli_rec.txt\", \"r\") as f:\n",
        "#   lines = f.readlines()\n",
        "# for line in lines:\n",
        "#   print(line)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "478q-4nRTlQc"
      },
      "outputs": [],
      "source": [
        "# test_num_ngs is the number of negative lines after each positive line in your test_file\n",
        "print(model.run_eval(test_file, num_ngs=test_num_ngs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkDH3m-iTlQd"
      },
      "source": [
        "AUC=0.5 is a state of random guess. We can see that before training, the model behaves like random guessing.\n",
        "\n",
        "#### 2.1 Train model\n",
        "Next we want to train the model on a training set, and check the performance on a validation dataset. Training the model is as simple as a function call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z2M3BZsTlQd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "with Timer() as train_time:\n",
        "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
        "\n",
        "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
        "# we will evaluate the performance of model on valid_file every epoch\n",
        "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7WtE9Y2TlQe"
      },
      "source": [
        "#### 2.2  Evaluate model\n",
        "\n",
        "Again, let's see what is the model's performance now (after training):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDtVLF1UTlQf"
      },
      "outputs": [],
      "source": [
        "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
        "print(res_syn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlDdJ9GdTlQg"
      },
      "outputs": [],
      "source": [
        "sb.glue(\"res_syn\", res_syn[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL3ravfhTlQh"
      },
      "source": [
        "If we want to get the full prediction scores rather than evaluation metrics, we can do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x7SuaA_TlQi"
      },
      "outputs": [],
      "source": [
        "model = model.predict(test_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xobQfJiTlQi"
      },
      "outputs": [],
      "source": [
        "# The data was downloaded in tmpdir folder. You can delete them manually if you do not need them any more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM547fPhTlQi"
      },
      "source": [
        "#### 2.3  Running models with large dataset\n",
        "Here are performances using the whole amazon dataset among popular sequential models with 1,697,533 positive instances.\n",
        "<br>Settings for reproducing the results:\n",
        "<br>`learning_rate=0.001, dropout=0.3, item_embedding_dim=32, cate_embedding_dim=8, l2_norm=0, batch_size=400, \n",
        "train_num_ngs=4, valid_num_ngs=4, test_num_ngs=49`\n",
        "\n",
        "\n",
        "We compare the running time with CPU only and with GPU on the larger dataset. It appears that GPU can significantly accelerate the training. Hardware specification for running the large dataset: \n",
        "<br>GPU: Tesla P100-PCIE-16GB\n",
        "<br>CPU: 6 cores Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz\n",
        " \n",
        "| Models | AUC | g-AUC | NDCG@2 | NDCG@10 | seconds per epoch on GPU | seconds per epoch on CPU| config |\n",
        "| :------| :------: | :------: | :------: | :------: | :------: | :------: | :------ |\n",
        "| A2SVD | 0.8251 | 0.8178 | 0.2922 | 0.4264 | 249.5 | 440.0 | N/A |\n",
        "| GRU4Rec | 0.8411 | 0.8332 | 0.3213 | 0.4547 | 439.0 | 4285.0 | max_seq_length=50, hidden_size=40|\n",
        "| Caser | 0.8244 | 0.8171 | 0.283 | 0.4194 | 314.3 | 5369.9 | T=1, n_v=128, n_h=128, L=3, min_seq_length=5|\n",
        "| SLi_Rec | 0.8631 | 0.8519 | 0.3491 | 0.4842 | 549.6 | 5014.0 | attention_size=40, max_seq_length=50, hidden_size=40|\n",
        "| NextItNet* | 0.6793 | 0.6769 | 0.0602 | 0.1733 | 112.0 | 214.5 | min_seq_length=3, dilations=\\[1,2,4,1,2,4\\], kernel_size=3 |\n",
        "| SUM | 0.8481 | 0.8406 | 0.3394 | 0.4774 | 1005.0 | 9427.0 | hidden_size=40, slots=4, dropout=0|\n",
        "\n",
        " Note 1: The five models are grid searched with a coarse granularity and the results are for reference only.\n",
        " <br>Note 2: NextItNet model requires a dataset with strong sequence property, but the Amazon dataset used in this notebook does not meet that requirement, so NextItNet Model may not performance good. If you wish to use other datasets with strong sequence property, NextItNet is recommended.\n",
        " <br>Note 3: Time cost of NextItNet Model is significantly shorter than other models because it doesn't need a history expanding of training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhRie-bLTlQj"
      },
      "source": [
        "## 3. Loading Trained Models\n",
        "In this section, we provide a simple example to illustrate how we can use the trained model to serve for production demand.\n",
        "\n",
        "Suppose we are in a new session. First let's load a previous trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibU_pTjLTlQk"
      },
      "outputs": [],
      "source": [
        "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
        "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
        "print('loading saved model in {0}'.format(path_best_trained))\n",
        "model_best_trained.load_model(path_best_trained)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_vNTzvgTlQl"
      },
      "source": [
        "Let's see if we load the model correctly. The testing metrics should be close to the numbers we have in the training stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV3gRKi0TlQm"
      },
      "outputs": [],
      "source": [
        "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52NkRjM-TlQn"
      },
      "source": [
        "And we make predictions using this model. In the next step, we will make predictions using a serving model. Then we can check if the two result files are consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdD-hMK-TlQo"
      },
      "outputs": [],
      "source": [
        "model_best_trained.predict(test_file, output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH6_gQKETlQp"
      },
      "source": [
        "## References\n",
        "\\[1\\] Zeping Yu, Jianxun Lian, Ahmad Mahmoody, Gongshen Liu, Xing Xie. Adaptive User Modeling with Long and Short-Term Preferences for Personailzed Recommendation. In Proceedings of the 28th International Joint Conferences on Artificial Intelligence, IJCAI19, Pages 4213-4219. AAAI Press, 2019.\n",
        "\n",
        "\\[2\\] Balzs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk. Session-based Recommendations with Recurrent Neural Networks. ICLR (Poster) 2016\n",
        "\n",
        "\\[3\\] Tang, Jiaxi, and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 2018.\n",
        "\n",
        "\\[4\\] Yuan, F., Karatzoglou, A., Arapakis, I., Jose, J. M., & He, X. A Simple Convolutional Generative Network for Next Item Recommendation. WSDM, 2019\n",
        "\n",
        "\\[5\\] Lian, J., Batal, I., Liu, Z., Soni, A., Kang, E. Y., Wang, Y., & Xie, X. Multi-Interest-Aware User Modeling for Large-Scale Sequential Recommendations. (2021) arXiv preprint arXiv:2102.09211."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK9wMT6DTlQq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMdxnULnBu2x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "collapsed_sections": [],
      "name": "sequential_recsys_amazondataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
